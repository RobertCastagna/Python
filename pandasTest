import re

import pandas as pd
import os
import numpy as np

df = pd.read_csv("sp500_companies.csv")
# print(df.head())

# clean table name : lowercase letters, remove spaces in names
file = "sp500_companies"
clean_table_name = file.lower().replace(" ", "")

# clean header of files : lowercase letters, remove spaces in names
df.columns = [x.lower().replace(" ", "") for x in df.columns]

# print(df[['symbol', 'shortname', 'currentprice']])
# print(df.iloc[1,1])

# for index, row in df.iterrows():
#     print(index, row['symbol'])

# find column with specific index value
# print(df.loc[df['symbol'] == "AAPL"])

# basic metrics of data
# print (df.describe())

# df = df[['symbol', 'currentprice']]
# print(df.sort_values(['currentprice'], ascending= 0))
# print(df.loc[df['symbol'] == 'AAPL'])

# combine columns (adding a new one)
# df['Total'] = df['shortname'] + ' ' + df['longname']
# or df['Total'] = df.iloc[:, 1:3].sum(axis=1) <- 1 is horizontal, 0 is vertical sum
# df = df.drop(columns='exchange')

# export shortened csv
# df = df[['symbol', 'shortname', 'currentprice', 'Total']]
# df.to_csv('modified_sp500.csv', index=0) #  use 'sep = "\t"' for tab seperator (i.e.)

# filtering
# print(df.columns)
# df = df.loc[(df['exchange'] == 'NYQ') & (df['currentprice'] > 100)]
# df = df.drop(df.iloc[:, 3:16], axis=1)
# df = df.loc[df['symbol'].str.contains('A')] # '~' for NOT operator
# df = df.loc[df['symbol'].str.contains('a|b', flags=re.I, regex=True)] # https://docs.python.org/3/library/re.html
# df.reset_index(drop=True, inplace=True)

# aggregate statistics
# df = df[['symbol', 'currentprice', 'exchange', 'marketcap', 'revenuegrowth']]
# df = df.groupby(['exchange']).mean().sort_values('revenuegrowth', ascending=0)
# count stocks per exchange
# df['count'] = 1
# df = df.groupby(['exchange']).count()['count']
# print(df)

# efficiently process huge datasets
# computes aggregate count of "symbol" column on each chunk of 5 rows imported
#new_df = pd.DataFrame(columns=df.columns)
# for df in pd.read_csv('modified_sp500.csv', chunksize=5):
#    results = df.groupby(['symbol']).count()
#    new_df = pd.concat([new_df, results])





